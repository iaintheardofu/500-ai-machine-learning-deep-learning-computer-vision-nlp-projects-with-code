{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install unsloth"
      ],
      "metadata": {
        "id": "oR7-LciUytoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AOWi0mRAyhTx",
        "outputId": "3abd12b6-e803-46b5-a678-57c9e19aee37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: UNSLOTH_COMPILE_DISABLE=1\n"
          ]
        }
      ],
      "source": [
        "# needed to fix a bug with unsloth\n",
        "%env UNSLOTH_COMPILE_DISABLE = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6kJh5LfnyhTy",
        "outputId": "3e8a5c70-fc82-44ae-e76a-04167083db50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'unsloth'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-3377921339>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0munsloth\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFastLanguageModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFastModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'unsloth'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel, FastModel\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import os\n",
        "import torch\n",
        "from torch import tensor\n",
        "import torch.nn.functional as F\n",
        "from transformers import TrainingArguments, Trainer, ModernBertModel, AutoModelForSequenceClassification, training_args\n",
        "from datasets import load_dataset, Dataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "model_name = 'answerdotai/ModernBERT-large'\n",
        "\n",
        "NUM_CLASSES = 3\n",
        "DATA_DIR = \"data/\"\n",
        "\n",
        "model, tokenizer = FastModel.from_pretrained(\n",
        "    model_name = model_name,load_in_4bit = False,\n",
        "    max_seq_length = 2048,\n",
        "    dtype = None,\n",
        "    auto_model = AutoModelForSequenceClassification,\n",
        "    num_labels = NUM_CLASSES,\n",
        ")\n",
        "print(\"model parameters:\" + str(sum(p.numel() for p in model.parameters())))\n",
        "\n",
        "# make all parameters trainable\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9GvlkzMyhTz"
      },
      "source": [
        "The dataset can be found [here](https://github.com/timothelaborie/text_classification_scripts/blob/main/data/finance_sentiment_multiclass.csv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Xn14jH_yhTz"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(DATA_DIR + \"finance_sentiment_multiclass.csv\")\n",
        "\n",
        "labels = data[\"label\"].tolist()\n",
        "labels = [x-1 for x in labels]\n",
        "# convert labels to one hot vectors\n",
        "labels = np.eye(NUM_CLASSES)[labels]\n",
        "\n",
        "train_data,val_data, train_labels, val_labels = train_test_split(data[\"text\"], labels, test_size=0.1, random_state=42)\n",
        "dataset = Dataset.from_list([{'text': text, 'labels': label} for text, label in zip(train_data, train_labels)])\n",
        "val_dataset = Dataset.from_list([{'text': text, 'labels': label} for text, label in zip(val_data, val_labels)])\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['text'])\n",
        "\n",
        "dataset = dataset.map(tokenize_function, batched=True)\n",
        "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTGsGOdZyhTz"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=32,\n",
        "        gradient_accumulation_steps=1,\n",
        "        warmup_steps=10,\n",
        "        fp16=not torch.cuda.is_bf16_supported(),\n",
        "        bf16=torch.cuda.is_bf16_supported(),\n",
        "        optim=training_args.OptimizerNames.ADAMW_TORCH,\n",
        "        learning_rate=5e-5,\n",
        "        weight_decay=0.001,\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        seed=3407,\n",
        "        num_train_epochs=3, # bert-style models usually need more than 1 epoch\n",
        "        save_strategy=\"epoch\",\n",
        "\n",
        "        # report_to=\"wandb\",\n",
        "        report_to=\"none\",\n",
        "\n",
        "        group_by_length=True,\n",
        "\n",
        "        # eval_strategy=\"no\",\n",
        "        eval_strategy=\"steps\",\n",
        "        eval_steps=0.25,\n",
        "        logging_strategy=\"steps\",\n",
        "        logging_steps=0.25,\n",
        "\n",
        "    ),\n",
        "    compute_metrics=lambda eval_pred: { \"accuracy\": accuracy_score(eval_pred[1].argmax(axis=-1), eval_pred[0].argmax(axis=-1)) }\n",
        ")\n",
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFbu7b7NyhT0"
      },
      "outputs": [],
      "source": [
        "model = model.cuda()\n",
        "model = model.eval()\n",
        "FastLanguageModel.for_inference(model)\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hSj49R9yhT0"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "correct = 0\n",
        "results = []\n",
        "\n",
        "# If the val_labels are one-hot, convert to class indices\n",
        "if isinstance(val_labels, np.ndarray) and val_labels.ndim == 2:\n",
        "    val_true_labels = np.argmax(val_labels, axis=1)\n",
        "else:\n",
        "    val_true_labels = val_labels\n",
        "\n",
        "val_texts = list(val_data)\n",
        "val_true_labels = list(val_true_labels)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in tqdm(range(0, len(val_texts), batch_size), desc=\"Evaluating\"):\n",
        "        batch_texts = val_texts[i:i+batch_size]\n",
        "        batch_labels = val_true_labels[i:i+batch_size]\n",
        "        # Tokenize\n",
        "        inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=2048)\n",
        "        inputs = {k: v.cuda() for k, v in inputs.items()}\n",
        "        # Forward pass\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        preds = torch.argmax(probs, dim=-1).cpu().numpy()\n",
        "        # Count correct\n",
        "        correct += np.sum(preds == batch_labels)\n",
        "        # Store results for display\n",
        "        for j in range(len(batch_texts)):\n",
        "            results.append({\n",
        "                \"text\": batch_texts[j][:200],\n",
        "                \"true\": batch_labels[j],\n",
        "                \"pred\": preds[j],\n",
        "                \"probs\": probs[j].detach().float().cpu().numpy(),\n",
        "                \"ok\": preds[j] == batch_labels[j]\n",
        "        })\n",
        "\n",
        "accuracy = 100 * correct / len(val_texts)\n",
        "print(f\"\\nValidation accuracy: {accuracy:.2f}% ({correct}/{len(val_texts)})\")\n",
        "\n",
        "# Show a few random samples\n",
        "import random\n",
        "display = 20\n",
        "print(\"\\n--- Random samples ---\")\n",
        "for s in random.sample(results, min(display, len(results))):\n",
        "    print(f\"\\nText: {s['text']}\")\n",
        "    print(f\"True: {s['true']}  Pred: {s['pred']} {'✅' if s['ok'] else '❌'}\")\n",
        "    print(\"Probs:\", \", \".join([f\"{k}: {v:.3f}\" for k, v in enumerate(s['probs'])]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9rCIDcNyhT0"
      },
      "outputs": [],
      "source": [
        "# stop running all cells\n",
        "1/0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sWzxiSOyhT0"
      },
      "source": [
        "# to load the model again (run every cell above the one where the trainer is called)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fOHdvRwxyhT0"
      },
      "outputs": [],
      "source": [
        "from transformers.trainer_utils import get_last_checkpoint\n",
        "\n",
        "output_dir = \"trainer_output\"\n",
        "last_checkpoint = get_last_checkpoint(output_dir)\n",
        "print(\"Last checkpoint:\", last_checkpoint)\n",
        "\n",
        "model, tokenizer = FastModel.from_pretrained(\n",
        "    model_name = last_checkpoint,load_in_4bit = False,\n",
        "    max_seq_length = 2048,\n",
        "    dtype = None,\n",
        "    auto_model = AutoModelForSequenceClassification,\n",
        "    num_labels = NUM_CLASSES,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDZGJjKEyhT1"
      },
      "outputs": [],
      "source": [
        "from torch import tensor\n",
        "print(model(input_ids=tensor([[1,2,3,4,5]]).cuda(), attention_mask=tensor([[1,1,1,1,1]]).cuda()))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "1be15a159d9874788f7b7854451912393d9e82d0d2bc47d83a870bda7fd9bc22"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}